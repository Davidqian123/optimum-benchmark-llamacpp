_target_: src.backend.onnxruntime.ORTBackend

name: onnxruntime
version: ${onnxruntime_version:} # resolved by onnxruntime config dataclass

provider: ${infer_provider:${device}}
use_io_binding: false

enable_profiling: ${is_profiling:${benchmark.name}}
inter_op_num_threads: null # for now default to ort's default
intra_op_num_threads: null # for now default to ort's default

optimization_level: null # O1, O2, O3, O4.
optimization_parameters: # additional optimization configs, for example:
  for_gpu: ${is_gpu:${device}}
  # some parameters such as transformers specific and fp16 are
  # omitted since they are handeled by auto optimization level
  disable_gelu_fusion: null
  disable_layer_norm_fusion: null
  disable_attention_fusion: null
  disable_skip_layer_norm_fusion: null
  disable_bias_skip_layer_norm_fusion: null
  disable_bias_gelu_fusion: null
  disable_embed_layer_norm_fusion: null
  use_mask_index: null
  # no_attention_mask: null # hard to use since it depends on the model
  disable_shape_inference: null

quantization_strategy: null # arm64, avx2, avx512, avx512_vnni, tensorrt.
quantization_parameters: # additional optimization configs, for example:
  is_static: null
  use_symmetric_activations: null
  use_symmetric_weights: null
  per_channel: null
