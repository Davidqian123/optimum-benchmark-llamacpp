_target_: src.backend.onnxruntime.ORTBackend

name: onnxruntime
version: ${onnxruntime_version:} # resolved by onnxruntime config dataclass

provider: ${infer_provider:${device}}
use_io_binding: false

enable_profiling: ${is_profiling:${benchmark.name}}
inter_op_num_threads: null # for now default to ort's default
intra_op_num_threads: null # for now default to ort's default

optimization:
  level: null # O1, O2, O3, O4
  for_gpu: ${is_gpu:${device}}
  # parameters such as transformers specific and fp16 are
  # omitted since they are handeled by optimum's optimization level
  disable_gelu_fusion: null # true, false
  disable_layer_norm_fusion: null # true, false
  disable_attention_fusion: null # true, false
  disable_skip_layer_norm_fusion: null # true, false
  disable_bias_skip_layer_norm_fusion: null # true, false
  disable_bias_gelu_fusion: null # true, false
  use_mask_index: null # true, false
  no_attention_mask: null # true, false
  disable_embed_layer_norm_fusion: null # true, false
  disable_shape_inference: null # true, false
  # experimental parameters
  use_multi_head_attention: null # true, false
  enable_gemm_fast_gelu_fusion: null # true, false
  use_raw_attention_mask: null # true, false
  disable_group_norm_fusion: null # true, false
  disable_packed_kv: null # true, false

quantization:
  instructions: null # arm64, avx2, avx512, avx512_vnni, tensorrt.
  is_static: null # true, false
  format: null # QOperator, QDQ
  mode: null # QLinearOps, IntegerOps
  activations_dtype: null # QInt8, QUInt8
  weights_dtype: null # QInt8, QUInt8
  per_channel: null # true, false
  reduce_range: null # true, false
  operators_to_quantize: null # list of strings
