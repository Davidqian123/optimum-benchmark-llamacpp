_target_: src.backend.onnxruntime.ORTBackend

name: onnxruntime
version: ${onnxruntime_version:} # resolved by onnxruntime config dataclass

provider: ${infer_provider:${device}}
use_io_binding: ${is_gpu:${device}}

enable_profiling: ${benchmark.profile}
inter_op_num_threads: null
intra_op_num_threads: null

optimization: false # true, false
optimization_config:
  optimization_level: 1 # 0, 1, 2, 99
  optimize_for_gpu: ${is_gpu:${device}}
  fp16: false
  enable_transformers_specific_optimizations: true
  enable_gelu_approximation: false
  disable_gelu_fusion: false
  disable_layer_norm_fusion: false
  disable_attention_fusion: false
  disable_skip_layer_norm_fusion: true
  disable_bias_skip_layer_norm_fusion: false
  disable_bias_gelu_fusion: false
  use_mask_index: false
  no_attention_mask: false
  disable_embed_layer_norm_fusion: true
  disable_shape_inference: false
  use_multi_head_attention: false
  enable_gemm_fast_gelu_fusion: false
  use_raw_attention_mask: false
  disable_group_norm_fusion: true
  disable_packed_kv: true

auto_optimization: null # O1, O2, O3, O4
auto_optimization_config:
  for_gpu: ${is_gpu:${device}}
  # add whatever you want to override

quantization: false # true, false
quantization_config:
  is_static: false # true, false
  format: QOperator # QOperator, QDQ
  mode: IntegerOps # QLinearOps, IntegerOps
  activations_dtype: QUInt8 # QInt8, QUInt8
  activations_symmetric: false # true, false
  weights_dtype: QInt8 # QInt8, QUInt8
  weights_symmetric: true # true, false
  per_channel: false # true, false
  reduce_range: false # true, false
  operators_to_quantize:
    - Gather
    - Transpose
    - EmbedLayerNormalization
    - Attention
    - LSTM
    - ArgMax
    - Conv
    - Gemm
    - MatMul
    - Add
    - Mul
    - Relu
    - Clip
    - LeakyRelu
    - Sigmoid
    - MaxPool
    - GlobalAveragePool
    - Split
    - Pad
    - Reshape
    - Squeeze
    - Unsqueeze
    - Resize
    - AveragePool
    - Concat
    - Softmax
    - Where
    - ConvTranspose
    - InstanceNormalization

auto_quantization: null # arm64,avx2,avx512,avx512_vnni,tensorrt
auto_quantization_config:
  is_static: false
  # add whatever you want to override
