experiment_name,backend.name,backend.version,backend._target_,backend.seed,backend.inter_op_num_threads,backend.intra_op_num_threads,backend.initial_isolation_check,backend.continous_isolation_check,backend.delete_cache,backend.no_weights,backend.device_map,backend.torch_dtype,backend.disable_grad,backend.eval_mode,backend.amp_autocast,backend.amp_dtype,backend.torch_compile,backend.bettertransformer,backend.quantization_strategy,backend.use_ddp,backend.peft_strategy,benchmark.name,benchmark._target_,benchmark.duration,benchmark.warmup_runs,benchmark.benchmark_duration,benchmark.memory,benchmark.energy,benchmark.input_shapes.batch_size,benchmark.input_shapes.sequence_length,benchmark.input_shapes.num_choices,benchmark.input_shapes.feature_size,benchmark.input_shapes.nb_max_frames,benchmark.input_shapes.audio_sequence_length,benchmark.new_tokens,benchmark.can_diffuse,benchmark.can_generate,benchmark.generate_kwargs.max_new_tokens,benchmark.generate_kwargs.min_new_tokens,benchmark.generate_kwargs.do_sample,benchmark.generate_kwargs.use_cache,benchmark.generate_kwargs.pad_token_id,benchmark.generate_kwargs.num_beams,model,device,task,hub_kwargs.revision,hub_kwargs.cache_dir,hub_kwargs.force_download,hub_kwargs.local_files_only,environment.optimum_version,environment.transformers_version,environment.accelerate_version,environment.diffusers_version,environment.python_version,environment.system,environment.cpu,environment.cpu_count,environment.cpu_ram_mb,environment.gpu,Unnamed: 0,forward.latency(s),forward.throughput(samples/s),forward.peak_memory(MB),generate.latency(s),generate.throughput(tokens/s),generate.peak_memory(MB),backend.quantization_config.load_in_8bit,backend.quantization_config.load_in_4bit,backend.quantization_config.llm_int8_threshold,backend.quantization_config.bnb_4bit_compute_dtype,backend.quantization_config.bits
llama_baseline,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,10,10,,True,False,4,512,1,80,3000,16000,,False,True,100,100,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.8.10,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0815,49.1,18791,3.74,107.0,18791,,,,,
llama_baseline,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,10,10,,True,False,16,512,1,80,3000,16000,,False,True,100,100,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.8.10,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.356,44.9,28119,4.55,352.0,28119,,,,,
llama_gptq,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,gptq,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,10,10,,True,False,16,512,1,80,3000,16000,,False,True,100,100,False,True,0,1,TheBloke/Llama-2-7B-GPTQ,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.8.10,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.415,38.6,18952,5.75,278.0,18952,,,,,4
llama_bnb,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,bnb,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,10,10,,True,False,16,512,1,80,3000,16000,,False,True,100,100,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.8.10,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.423,37.8,18921,7.1,225.0,18921,False,True,0.0,float16,
llama_bnb,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,bnb,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,10,10,,True,False,4,512,1,80,3000,16000,,False,True,100,100,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.8.10,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.107,37.4,9687,7.13,56.1,9687,False,True,0.0,float16,
llama_gptq,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,gptq,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,10,10,,True,False,4,512,1,80,3000,16000,,False,True,100,100,False,True,0,1,TheBloke/Llama-2-7B-GPTQ,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.8.10,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.113,35.4,9624,3.15,127.0,9624,,,,,4
llama_baseline,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,10,10,,True,False,1,512,1,80,3000,16000,,False,True,100,100,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.8.10,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0356,28.1,16463,3.25,30.8,16463,,,,,
llama_gptq,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,gptq,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,10,10,,True,False,1,512,1,80,3000,16000,,False,True,100,100,False,True,0,1,TheBloke/Llama-2-7B-GPTQ,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.8.10,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0368,27.2,7053,2.91,34.4,7242,,,,,4
llama_bnb,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,bnb,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,10,10,,True,False,1,512,1,80,3000,16000,,False,True,100,100,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.8.10,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0699,14.3,7416,5.33,18.8,7418,False,True,0.0,float16,
