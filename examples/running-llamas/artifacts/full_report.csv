experiment_name,backend.name,backend.version,backend._target_,backend.seed,backend.inter_op_num_threads,backend.intra_op_num_threads,backend.initial_isolation_check,backend.continous_isolation_check,backend.delete_cache,backend.no_weights,backend.device_map,backend.torch_dtype,backend.disable_grad,backend.eval_mode,backend.amp_autocast,backend.amp_dtype,backend.torch_compile,backend.bettertransformer,backend.quantization_strategy,backend.use_ddp,backend.peft_strategy,benchmark.name,benchmark._target_,benchmark.memory,benchmark.duration,benchmark.warmup_runs,benchmark.benchmark_duration,benchmark.input_shapes.batch_size,benchmark.input_shapes.sequence_length,benchmark.input_shapes.num_choices,benchmark.input_shapes.feature_size,benchmark.input_shapes.nb_max_frames,benchmark.input_shapes.audio_sequence_length,benchmark.new_tokens,benchmark.can_diffuse,benchmark.can_generate,benchmark.generate_kwargs.max_new_tokens,benchmark.generate_kwargs.min_new_tokens,benchmark.generate_kwargs.do_sample,benchmark.generate_kwargs.use_cache,benchmark.generate_kwargs.pad_token_id,benchmark.generate_kwargs.num_beams,model,device,task,hub_kwargs.revision,hub_kwargs.cache_dir,hub_kwargs.force_download,hub_kwargs.local_files_only,environment.optimum_version,environment.transformers_version,environment.accelerate_version,environment.diffusers_version,environment.python_version,environment.system,environment.cpu,environment.cpu_count,environment.cpu_ram_mb,environment.gpu,Unnamed: 0,forward.latency(s),forward.throughput(samples/s),forward.peak_memory(MB),generate.latency(s),generate.throughput(tokens/s),backend.quantization_config.load_in_8bit,backend.quantization_config.load_in_4bit,backend.quantization_config.llm_int8_threshold,backend.quantization_config.bnb_4bit_compute_dtype,backend.quantization_config.bits
llama_baseline,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,True,10,10,,16,16,1,80,3000,16000,1000,False,True,1000,1000,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.9.17,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0287,557.0,4424,36.0,444.0,,,,,
llama_gptq,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,gptq,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,True,10,10,,16,16,1,80,3000,16000,1000,False,True,1000,1000,False,True,0,1,TheBloke/Llama-2-7B-GPTQ,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.9.17,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0334,479.0,4424,48.3,331.0,,,,,4
llama_bnb,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,bnb,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,True,10,10,,16,16,1,80,3000,16000,1000,False,True,1000,1000,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.9.17,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0534,300.0,4424,61.9,258.0,False,True,0.0,float16,
llama_baseline,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,True,10,10,,1,16,1,80,3000,16000,1000,False,True,1000,1000,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.9.17,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0288,34.7,7411,28.0,35.7,,,,,
llama_gptq,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,gptq,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,True,10,10,,1,16,1,80,3000,16000,1000,False,True,1000,1000,False,True,0,1,TheBloke/Llama-2-7B-GPTQ,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.9.17,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0316,31.6,4424,27.7,36.1,,,,,4
llama_bnb,pytorch,2.0.1,optimum_benchmark.backends.pytorch.backend.PyTorchBackend,42,,,True,True,False,False,,float16,True,True,False,,False,False,bnb,False,,inference,optimum_benchmark.benchmarks.inference.benchmark.InferenceBenchmark,True,10,10,,1,16,1,80,3000,16000,1000,False,True,1000,1000,False,True,0,1,meta-llama/Llama-2-7b-hf,cuda:0,text-generation,main,,False,False,1.12.1.dev0,4.33.0.dev0,0.23.0.dev0,0.21.0.dev0,3.9.17,Linux, AMD EPYC 7742 64-Core Processor,128,540684,NVIDIA A100-SXM4-80GB,0,0.0525,19.0,4424,40.4,24.8,False,True,0.0,float16,
