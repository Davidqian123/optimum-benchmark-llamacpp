defaults:
  - base_config # inherits from base config

experiment_name: peft_bettertransformer_4bit
model: meta-llama/Llama-2-7b-hf
device: cuda:0

backend:
  no_weights: true
  device_map: auto
  quantization_strategy: bnb
  quantization_config:
    load_in_4bit: true
  bettertransformer: true
  peft_strategy: lora
  peft_config:
    task_type: CAUSAL_LM

benchmark:
  warmup_steps: 10
  dataset_shapes:
    dataset_size: 1200
    sequence_length: 256
  training_arguments:
    max_steps: 20
    per_device_train_batch_size: 8