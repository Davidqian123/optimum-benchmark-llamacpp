defaults:
  - base_config # inherits from base config

experiment_name: peft_gptq
model: TheBloke/Llama-2-7B-GPTQ
device: cuda:0

backend:
  peft_strategy: lora
  peft_config:
    task_type: CAUSAL_LM

benchmark:
  warmup_steps: 10
  dataset_shapes:
    dataset_size: 80
    sequence_length: 256
  training_arguments:
    per_device_train_batch_size: 8
